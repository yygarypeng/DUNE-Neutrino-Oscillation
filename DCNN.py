{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nimport gc\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# check the tensorflow and GPU\n\n# print(f\"Using Tensorflow {tf.__version__}\")\n# device_name = tf.test.gpu_device_name()\n# if device_name != \"/device:GPU:0\":\n#     raise SystemError(\"GPU device not found\")\n# print(\n#     f\"Found GPU adn CPU.\\nTensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\"\n# )\n\n\n# import the datasets and check the files\n\nimport os\n\n# path = \"./phys591000-2023-final-project/\"  # run on the local machine\n# path = \"/content/drive/Shareddrives/2023AI_final/2023AI_final/phys591000-2023-final-project/\" # run on the google colab\npath = \"/kaggle/input/dune-neutrino/\"  # Kaggle\nif not os.path.isfile(path + \"neutrino_test_data.npz\") or os.path.isfile(\n    path + \"neutrino_train_data.npz\"\n):\n    raise FileNotFoundError(\"test/train data was not found or is a directory\")\n\n# take out data from the datasets\n\ndata_train = np.load(path + \"neutrino_training_data.npz\")  # ideal data\ndata_test = np.load(path + \"neutrino_test_data.npz\")  # pseudo-exp data\n\nname_train = data_train.files\nname_test = data_test.files\n\n(\n    ve_train,\n    vebar_train,\n    vu_train,\n    vubar_train,\n    theta23_train,\n    delta_train,\n    ldm_train,\n) = map(lambda n: data_train[n], name_train)\nve_test, vebar_test, vu_test, vubar_test = map(lambda n: data_test[n], name_test)\n\n\n# create train and test data\nX_train = np.stack((ve_train, vebar_train, vu_train, vubar_train), axis=-1)\nY_train = np.stack((theta23_train, delta_train, ldm_train), axis=-1)\n# X_test = np.stack((ve_test, vebar_test, vu_test, vubar_test), axis=-1)\n\n# [X] normalize training data to [0,1]\nx_train_NH = X_train[ldm_train > 0]\nprint(f\"Before normalized, the shape of x_train: {x_train_NH.shape}\")\nv_max = np.max(x_train_NH)\nv_min = np.min(x_train_NH)\nprint(f\"X-train normalized factors (v_max, v_min) = ({v_max}, {v_min})\")\nx_train_NH_norm = (x_train_NH - v_min) / (v_max - v_min)\nprint(f\"After normalized, the shape of x_train: {x_train_NH_norm.shape}\")\n# [Y] normalize training label to [0,1]\ny_train_NH = Y_train[:, 0][ldm_train > 0]\nprint(f\"Before normalized, the shape of y_train: {y_train_NH.shape}\")\nv_max = np.max(y_train_NH)\nv_min = np.min(y_train_NH)\ny_train_NH_norm = (y_train_NH - v_min) / (v_max - v_min)\nprint(f\"Y-train normalized factors (v_max, v_min) = ({v_max}, {v_min})\")\nprint(f\"After normalized, the shape of y_train: {y_train_NH_norm.shape}\")\n\n# clear unused variables\ndel (\n    X_train,\n    Y_train,\n    ve_train,\n    vebar_train,\n    vu_train,\n    vubar_train,\n    theta23_train,\n    delta_train,\n    ldm_train,\n    ve_test,\n    vebar_test,\n    vu_test,\n    vubar_test,\n)\ngc.collect()\n\n# create validation data\nfrom sklearn.model_selection import train_test_split\n\n# split the training dataset into training and validation, with test_size = 0.2\ntf.random.set_seed(2023)\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_NH_norm,\n    y_train_NH_norm,\n    test_size=0.2,\n    shuffle=True,\n)\n# clear unused variables\ndel x_train_NH_norm, y_train_NH_norm\n\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import (\n    AveragePooling2D,\n    BatchNormalization,\n    Conv2D,\n    Conv2DTranspose,\n    Dense,\n    Dropout,\n    Flatten,\n    Lambda,\n    Layer,\n    LeakyReLU,\n    Reshape,\n)\n\n\ndef create_model_cvae(input_dim, latent_dim):\n    def conv2d(inputs, filters, kernel_size):\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(inputs)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = AveragePooling2D(pool_size=(2, 2))(x)\n        return x\n\n    # Encoder\n    inputs = Input(shape=input_dim)\n    x = conv2d(inputs, filters=64, kernel_size=4)\n    x = conv2d(x, filters=32, kernel_size=4)\n    x = Flatten()(x)\n    x = Dense(256, kernel_regularizer=regularizers.l2(0.002), activation=\"elu\")(x)\n    x = Dense(64, kernel_regularizer=regularizers.l2(0.002), activation=\"elu\")(x)\n    x = Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"elu\")(x)\n    x = Dense(1, activation=\"relu\")(x)\n    dcnn = Model(inputs, x, name=\"dnn\")\n    dcnn.summary()\n\n    # CVAE + DNN\n    model_inputs = Input(shape=input_dim)\n    fin_dnn = dcnn(model_inputs)\n    dcnn = Model(inputs=model_inputs, outputs=fin_dnn)\n    return dcnn\n\n\nfrom tensorflow.keras.optimizers import Adam\n\n# Declare the model\ncvae = create_model_cvae(\n    input_dim=(x_train.shape[1], x_train.shape[2], 1), latent_dim=2\n)\n\n# Compile the model\ncvae.compile(optimizer=Adam(5e-6), loss=\"huber\")\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# train\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.005,\n    patience=30,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=False,\n)\n\ncvae.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    epochs=256,\n    batch_size=64,\n    callbacks=[early_stopping],\n    shuffle=True,\n    verbose=2,\n)\n\n# check the loss function\nfig = plt.figure(figsize=(8, 5), dpi=120)\nhistory = cvae.history.history\nplt.plot(history[\"loss\"], lw=2.5, label=\"Train\", alpha=0.8)\nplt.plot(history[\"val_loss\"], lw=2.5, label=\"Validation\", alpha=0.8)\nplt.title(\"Epoch vs Huber loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss (Huber)\")\nplt.legend(loc=\"best\")\nplt.savefig(\"CVAE_loss.png\")\nplt.close()\n\n# save model\ncvae.save(\"./CVAE_Theta23.h5\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-25T12:41:42.863762Z","iopub.execute_input":"2023-05-25T12:41:42.864158Z","iopub.status.idle":"2023-05-25T22:18:16.839291Z","shell.execute_reply.started":"2023-05-25T12:41:42.864130Z","shell.execute_reply":"2023-05-25T22:18:16.837387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Neutrino Oscillation with DUNE**\n\n## PHYS591000 Physics AI Final Project\n\nThis is the project implementing ML to find out the $\\Theta_{13}$ and $\\delta_{cp}$ using the neutrino data from DUNE experiment.  \nOriginal link: [Kaggle](https://www.kaggle.com/competitions/phys591000-2023-final-project-i)\n\n> Author(s): Siang-Yuan Lin, Yuan-Yen Peng  \n> Email(s) : sylin@gapp.nthu.edu.tw, garyphys0915@gapp.nthu.edu.tw  \n> Licence : MIT  \n> Data : May, 2023\n","metadata":{"id":"-elmT41k47Qv"}},{"cell_type":"markdown","source":"### prerequisites\n\n1. reset all varialbes\n2. import curcial modules\n3. mount google drive\n4. check the GPU is on\n","metadata":{"id":"rvTBPoaX5_UT"}},{"cell_type":"code","source":"# clear all variables\n%reset -f ","metadata":{"id":"2mREXgO8yDlZ","outputId":"352144e9-3c4a-49b9-bf80-684ed940dad1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\n\nfrom numba import njit  # acceleration kernel\nimport tensorflow as tf","metadata":{"id":"PDGlck4FyDlb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # mount the google drive to retrieve dataset\n\n# from google.colab import drive\n\n# drive.mount('/content/drive')","metadata":{"id":"kMiiyodAyuPP","outputId":"cfbd05e6-913c-4b7b-98ab-6e16371c1a85","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the tensorflow and GPU\n\nprint(f\"Using Tensorflow {tf.__version__}\")\ndevice_name = tf.test.gpu_device_name()\nif device_name != \"/device:GPU:0\":\n    raise SystemError(\"GPU device not found\")\nprint(\n    f\"Found GPU adn CPU.\\nTensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\"\n)\n","metadata":{"id":"y2-tb7UfyDlb","outputId":"697db891-b70c-4151-f91a-0ca2c5de46be","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### import datasets\n\n1. check files exist\n2. retrieve data\n3. check shapes of data\n","metadata":{"id":"OhDvdQ666DPJ"}},{"cell_type":"code","source":"# import the datasets and check the files\n\nimport os\n\n# path = \"./phys591000-2023-final-project/\"  # run on the local machine\n# path = \"/content/drive/Shareddrives/2023AI_final/2023AI_final/phys591000-2023-final-project/\" # run on the google colab\npath = \"/kaggle/input/phys591000-2023-final-project-i/\"# Kaggle\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        print(\"-> The file is\", filename)\nif not os.path.isfile(path + \"neutrino_test_data.npz\") or os.path.isfile(\n    path + \"neutrino_train_data.npz\"\n):\n    raise FileNotFoundError(\"test/train data was not found or is a directory\")\n","metadata":{"id":"MUyLieJzyDlc","outputId":"264b6570-6890-4f28-d6cd-ef5ea1dc74b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# take out data from the datasets\n\ndata_train = np.load(path + \"neutrino_training_data.npz\")  # ideal data\ndata_test = np.load(path + \"neutrino_test_data.npz\")  # pseudo-exp data\n\nname_train = data_train.files\nname_test = data_test.files\n\nprint(f\"Train: {name_train}\")\nprint(f\"Test : {name_test}\")\n\nve_train, vebar_train, vu_train, vubar_train, theta23_train, delta_train, ldm_train = map(lambda n: data_train[n], name_train)\nve_test, vebar_test, vu_test, vubar_test = map(lambda n: data_test[n], name_test)","metadata":{"id":"OsrT7ptcyDld","outputId":"84d6e488-3f54-48aa-b7c4-b1dd10df55d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the data shape\n\nprint(\n    f\"\"\"# TRAIN\nve_train     :{ve_train.shape}\nvebar_train  :{vebar_train.shape}\nvu_train     :{vu_train.shape}\nvubar_train  :{vubar_train.shape}\ntheta23_train:{theta23_train.shape}\ndelta_train  :{delta_train.shape}\nldm_train    :{ldm_train.shape}\n\"\"\"\n)\nprint(\n    f\"\"\"# TEST\nve_test      :{ve_test.shape}\nvebar_test   :{vebar_test.shape}\nvu_test      :{vu_test.shape}\nvubar_test   :{vubar_test.shape}\n\"\"\"\n)\n","metadata":{"id":"N3-WvJIiyDle","outputId":"1bf6b586-9a59-4b79-bf52-f4b949c8242c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### preprocessing\n\n1. create train and test data\n2. inspect data structure\n3. check data\n4. normalize data\n5. <pending>\n","metadata":{"id":"xOEy8aPW6Gim"}},{"cell_type":"code","source":"# create train and test data\nX_train = np.stack((ve_train, vebar_train, vu_train, vubar_train), axis=-1)\nY_train = np.stack((theta23_train, delta_train, ldm_train), axis=-1)\nX_test = np.stack((ve_test, vebar_test, vu_test, vubar_test), axis=-1)\n\nprint(\n    f\"\"\"\nThe shape of x_train: {X_train.shape}\nThe shape of y_train: {Y_train.shape}\nThe shape of x_test : {X_test.shape}\n\"\"\"\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define plotting functions\n\n\ndef four_plot(data_format: list) -> None:\n    train, test = data_format\n    bins = np.arange(0.625, 8 + 0.125, 0.125)\n    name = [r\"$\\nu_e$\", r\"$\\bar{\\nu}_e$\", r\"$\\nu_\\mu$\", r\"$\\bar{\\nu}_\\mu$\"]\n\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10), dpi=80, constrained_layout=True)\n    fig.suptitle(\"First 60 features\")\n    for i, (train_data, test_data, name) in enumerate(zip(train, test, name)):\n        ax = axes[i // 2, i % 2]\n        ax.set_title(name)\n        ax.step(bins, train_data[:60], label=\"train\")\n        ax.step(bins, test_data[:60], label=\"test\")\n        ax.legend(loc=\"best\")\n        ax.set_xlabel(\"Energy E [GeV]\")\n        ax.set_ylabel(r\"Flux density $\\phi\\ [kg\\cdot s^{-1}\\cdot m^{-2}]$\")\n\n    plt.show()\n    print(\"Success to plot\")\n    return\n\n\ndef four_2d_plot(data_format: list) -> None:\n    name = [r\"$\\nu_e$\", r\"$\\bar{\\nu}_e$\", r\"$\\nu_\\mu$\", r\"$\\bar{\\nu}_\\mu$\"]\n    vmin = min(np.min(data_format[0]), np.min(data_format[1]))\n    vmax = max(np.max(data_format[0]), np.max(data_format[1]))\n    dataset_name = [\"train\", \"test\"]\n    fig, axes = plt.subplots(2, 1, figsize=(10, 4), dpi=80, constrained_layout=True)\n    fig.suptitle(\"First 60 features\")\n    for i, data in enumerate(data_format):\n        ax = axes[i]\n        im = ax.imshow(data.T, origin=\"lower\")\n        ax.set_title(dataset_name[i])\n        ax.set_xlabel(\"Energy E [GeV]\")\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(name)\n        im.set_clim(vmin, vmax)\n    cbar = fig.colorbar(im, ax=axes, location=\"bottom\", aspect=25, pad=0.1)\n    cbar.ax.set_title(r\"Flux density $\\phi\\ [kg\\cdot s^{-1}\\cdot m^{-2}]$\")\n    plt.show()\n    print(\"Success to plot\")\n    return\n","metadata":{"id":"Mn2pr4Eop4B7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect data structures\n\n# check ``1D images''\ntrain_10 = [ve_train[10], vebar_train[10], vu_train[10], vubar_train[10]]\ntest_10 = [ve_test[10], vebar_test[10], vu_test[10], vubar_test[10]]\ntrain_average = [\n    np.average(ve_train, axis=0),\n    np.average(vebar_train, axis=0),\n    np.average(vu_train, axis=0),\n    np.average(vubar_train, axis=0),\n]\ntest_average = [\n    np.average(ve_test, axis=0),\n    np.average(vebar_test, axis=0),\n    np.average(vu_test, axis=0),\n    np.average(vubar_test, axis=0),\n]\ndata_format = [train_10, test_10]\ndata_sum_format = [train_average, test_average]\nfour_plot(data_format)\nfour_plot(data_sum_format)\n\n# check ``2D images''\nfor i in [9, 99, 999]:\n    data_format = [X_train[i, 0:60, :], X_test[i, 0:60, :]]\n    four_2d_plot(data_format)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check theta and delta relations\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 7), dpi=100, constrained_layout=True)\nfor i, name in enumerate([r\"$\\Theta_{13}$ [rad]\", r\"$\\delta_{cp}$ [rad]\"]):\n    print(\n        f\"\"\"The max of {name} is {np.max(Y_train[:, i])};\nThe min of {name} is {np.min(Y_train[:, i])};\nThe avg of {name} is {np.average(Y_train[:, i])}.\n\"\"\"\n    )\n    ax = axes[i]\n    n = 100\n    ax.bar(np.arange(n), Y_train[:n, i], width=0.1)\n    ax.set_title(f\"First 100 features of {name}\")\n    ax.set_xlabel(\"order of features\")\n    ax.set_ylabel(\"Values [unit]\")\nplt.show()\n","metadata":{"id":"JNevuhBmSLuc","outputId":"bf9f2a50-1b59-49f0-e670-2f3da189b03b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation data\nfrom sklearn.model_selection import train_test_split\n\n# split the training dataset into training and validation, with test_size = 0.2\ntf.random.set_seed(2023)\nx_train, x_val, y_train, y_val = train_test_split(\n    X_train, Y_train[:, 0:2], test_size=0.2, shuffle=True\n)\nx_test = X_test\nprint(\n    f\"\"\"\nThe shape of x_train     : {x_train.shape}\nThe shape of y_train     : {y_train.shape}\nThe shape of x_validation: {x_val.shape}\nThe shape of y_validation: {y_val.shape}\nThe shape of x_test      : {x_test.shape}\n\"\"\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model\n","metadata":{}},{"cell_type":"code","source":"# Define sample_z function\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\ndef sample_z(inputs: list):\n    z_mean, z_log_var = inputs\n    batch = tf.shape(z_mean)[0]\n    dim = tf.shape(z_mean)[1]\n    epsilon = K.random_normal(shape=(batch, dim))\n\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KLDivergenceLayer(Layer):\n\n    \"\"\"Identity transform layer that adds KL divergence\n    to the final model loss.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.is_placeholder = True\n        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n\n    def call(self, inputs):\n        mu, log_var = inputs\n        kl_batch = -0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n        self.add_loss(K.mean(kl_batch), inputs=inputs)\n\n        return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import regularizers\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import (\n    Reshape,\n    Conv2D,\n    MaxPooling2D,\n    Conv2DTranspose,\n    LeakyReLU,\n    Flatten,\n    Dense,\n    Lambda,\n    Dropout,\n    BatchNormalization,\n)\n\n\ndef conv2d(inputs,kernel_size):\n    x = Conv2D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(inputs)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Flatten()(x)\n    return x\n\n\ndef deconv2d(inputs, kernel_size):\n    x = Conv2DTranspose(1, kernel_size=kernel_size, strides=2, padding=\"same\")(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Flatten()(x)\n    return x\n\n\ndef create_vae_model(input_dim, latent_dim):\n    # Encoder\n    encoder_inputs = Input(shape=input_dim)\n    x = conv2d(encoder_inputs, 8)\n    x = Dense(64, activation=\"elu\")(x)\n    x = Reshape((8, 8, 1))(x)\n    x = conv2d(x, 4)\n    x = Dense(4, activation=\"elu\")(x)\n    # latent space\n    z_mu = Dense(latent_dim)(x)\n    z_log_var = Dense(latent_dim)(x)\n    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n    z = Lambda(sample_z, output_shape=(latent_dim,))([z_mu, z_log_var])\n    encoder = Model(encoder_inputs, [z_mu, z_log_var, z], name=\"encoder\")\n    encoder.summary()\n\n    # Decoder\n    latent_inputs = Input(shape=(latent_dim,))\n    x = Dense(4, activation=\"elu\")(latent_inputs)\n    x = Reshape((2, 2, 1))(x)\n    x = deconv2d(x, 4)\n    x = Dense(64, activation=\"elu\")(x)\n    x = Dropout(0.3)(x)\n    x = Reshape((8, 8, 1))(x)\n    x = deconv2d(x, 64)\n    x = Dense(np.prod(input_dim), activation=\"elu\")(x)\n    x = Dropout(0.3)(x)\n    decoder = Model(latent_inputs, x, name=\"decoder\")\n    decoder.summary()\n    \n    # DNN (conti --> decoder)\n    inputs = Input(shape=x.shape)\n    x = Dense(64, activation=\"elu\", kernel_regularizer=regularizers.l2(l=0.01))(inputs)\n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"elu\", kernel_regularizer=regularizers.l2(l=0.01))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation=\"linear\", kernel_regularizer=regularizers.l2(l=0.01))(x)\n    dnn = Model(inputs, x, name=\"dnn\")\n    dnn.summary()\n\n    # VAE\n    autoencoder_input = Input(shape=input_dim)\n    encoded = encoder(autoencoder_input)\n    decoded = decoder(encoded[2])\n    fin_dnn = dnn(decoded)\n    vae = Model(inputs=autoencoder_input, outputs=fin_dnn)\n    return vae\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare the model and encoder\n# vae = create_vae_model(input_dim=(x_train.shape[1], x_train.shape[2], 1), latent_dim=2)\nvae = create_vae_model(\n    input_dim=(x_train.shape[1], x_train.shape[2], 1), latent_dim=2\n)\n\n# Compile the model\nvae.compile(optimizer=\"adam\", loss=\"mse\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.01,\n    patience=20,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=False,\n)\nvae.fit(\n    x=x_train,\n    y=y_train[:, 0],\n    validation_data=(x_val, y_val[:, 0]),\n    epochs=128,\n    batch_size=512,\n    callbacks=[early_stopping],\n    shuffle=True,\n    verbose=2,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5), dpi=100)\nhistory = vae.history.history\nplt.plot(history[\"loss\"], lw=2.5, label=\"Train\", alpha=0.8)\nplt.plot(history[\"val_loss\"], lw=2.5, label=\"Validation\", alpha=0.8)\nplt.title(\"Epoch vs MSE loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss (MSE)\")\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\nx_train_pred = vae.predict(x_train)\nx_test_pred  = vae.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistical parameters\nn = len(x_train_pred[:,0])\ndof = n - 2\nt = scipy.stats.t.ppf(0.975, dof)\n\n# linear fitting line\nslope, intercept = np.polyfit(x_train_pred[:,0], y_train[:,0], 1)\nplt_min = np.min([np.min(x_train_pred), np.min(y_train[:,0])])\nplt_max = np.max([np.max(x_train_pred), np.max(y_train[:,0])])\nx_line = np.linspace(plt_min, plt_max, 100)\ny_line = np.polyval([slope, intercept], x_line)\n\n# acceleration kernel (to calculate mse, confident level, and prediction level)\n@njit(fastmath=True)\ndef mse_kernel(pred, true, t=t, n=n, dof=dof, x_line=x_line, y_line=y_line):\n    if true.shape != pred.shape:\n        raise ValueError(\"True/Pred data should be the same shape!\") \n    mse = np.empty(len(true))\n    for ind, (t, p) in enumerate(zip(true, pred)):\n        mse[ind] = np.average(np.square(t - p))\n    pred_mean = np.average(x_train_pred[:,0])\n    sum_se = np.sum(np.square(true - pred))\n    std_error = np.sqrt(1/dof * sum_se)\n    # confidence interval\n    ci = t * std_error * np.sqrt(1/n + np.square(x_line - pred_mean)/np.sum(np.square(x_train_pred[:,0] - pred_mean)))\n    # predicting interval\n    pi = t * std_error * np.sqrt(1 + 1/n + np.square(x_line - pred_mean)/np.sum(np.square(x_train_pred[:,0] - pred_mean)))\n    return mse, ci, pi\n\nmse, ci, pi = mse_kernel(x_train_pred[:,0], y_train[:,0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(7,7), dpi=80)\nplt.scatter(x_train_pred, y_train[:,0], s=3, alpha=.2)\nplt.plot(x_line, y_line, color = 'red')\nplt.plot(plt_min, plt_max, '--', lw=1, c='orange')\nplt.plot(x_line, y_line + pi, '--', lw=1, c='tab:red', label='95% prediction interval')\nplt.plot(x_line, y_line - pi, '--', lw=1, c='tab:red')\nplt.plot(x_line, y_line + ci, '--', c='tab:grey', label='95% confidence interval')\nplt.plot(x_line, y_line - ci, '--', c='tab:grey')\nplt.xlim(plt_min, plt_max)\nplt.ylim(plt_min, plt_max)\nplt.xlabel(\"Pred\")\nplt.ylabel(\"True\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### dump the predicted data\n","metadata":{}},{"cell_type":"code","source":"# # dump the predicted data\n# \"\"\"\n# 0 for qcd; 1 for wprime\n# \"\"\"\n\n# pred_label = [0 if i < cut else 1 for i in new_loss_data]\n# np.savetxt(\"submission.csv\", [('id', 'prediction')] + [(i, j) for i, j in enumerate(pred_label)], delimiter=\",\", fmt='%s')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
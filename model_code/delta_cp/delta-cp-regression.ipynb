{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# check the tensorflow and GPU\nprint(f\"Using Tensorflow {tf.__version__}\")\ndevice_name = tf.test.gpu_device_name()\nif device_name != \"/device:GPU:0\":\n    raise SystemError(\"GPU device not found\")\nprint(\n    f\"Found GPU adn CPU.\\nTensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\"\n)\n\n\n# import the datasets and check the files\nimport os\n\n# path = \"./phys591000-2023-final-project/\"  # run on the local machine\n# path = \"/content/drive/Shareddrives/2023AI_final/2023AI_final/phys591000-2023-final-project/\" # run on the google colab\npath = \"/kaggle/input/dune-neutrino/\"  # Kaggle\nif not os.path.isfile(path + \"neutrino_test_data.npz\") or os.path.isfile(\n    path + \"neutrino_train_data.npz\"\n):\n    raise FileNotFoundError(\"test/train data was not found or is a directory\")\n\n# take out data from the datasets\n\ndata_train = np.load(path + \"neutrino_training_data.npz\")  # ideal data\ndata_test = np.load(path + \"neutrino_test_data.npz\")  # pseudo-exp data\n\nname_train = data_train.files\nname_test = data_test.files\n\n(\n    ve_train,\n    vebar_train,\n    vu_train,\n    vubar_train,\n    theta23_train,\n    delta_train,\n    ldm_train,\n) = map(lambda n: data_train[n], name_train)\nve_test, vebar_test, vu_test, vubar_test = map(lambda n: data_test[n], name_test)\n\n\n# create train and test data\nX_train = np.stack((ve_train, vebar_train, vu_train, vubar_train), axis=-1)\nY_train = np.stack((theta23_train, delta_train, ldm_train), axis=-1)\n# X_test = np.stack((ve_test, vebar_test, vu_test, vubar_test), axis=-1)","metadata":{"_uuid":"ae2b75bd-0d8e-44e8-864e-125b27363cc0","_cell_guid":"a2bc6e90-179a-4402-ac35-ff7a23fa7640","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [X] normalize training data to [0,1]\nx_train_NH = X_train[ldm_train > 0]\nprint(f\"Before normalized, the shape of x_train: {x_train_NH.shape}\")\nv_max = np.max(x_train_NH)\nv_min = np.min(x_train_NH)\nprint(f\"X-train normalized factors (v_max, v_min) = ({v_max}, {v_min})\")\nx_train_NH_norm = (x_train_NH - v_min) / (v_max - v_min)\nprint(f\"After normalized, the shape of x_train: {x_train_NH_norm.shape}\")\n\n# [Y] normalize training label to [0,1] --> using sine function \ny_train_NH = Y_train[:, 1][ldm_train > 0]\nprint(f\"Before normalized, the shape of y_train: {y_train_NH.shape}\")\n# v_max = np.max(y_train_NH)\n# v_min = np.min(y_train_NH)\ny_train_NH_norm = np.sin(y_train_NH * np.pi/180) # degree -> rad\n# print(f\"Y-train normalized factors (v_max, v_min) = ({v_max}, {v_min})\")\nprint(f\"After normalized, the shape of y_train: {y_train_NH_norm.shape}\")\n\n# clear unused variables\n# del (\n#     X_train,\n#     Y_train,\n#     ve_train,\n#     vebar_train,\n#     vu_train,\n#     vubar_train,\n#     theta23_train,\n#     delta_train,\n#     ldm_train,\n#     ve_test,\n#     vebar_test,\n#     vu_test,\n#     vubar_test,\n# )\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# create validation data\nfrom sklearn.model_selection import train_test_split\n\n# split the training dataset into training and validation, with test_size = 0.2\ntf.random.set_seed(2023)\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_NH_norm,\n    y_train_NH_norm,\n    test_size=0.2,\n    shuffle=True,\n)\n# clear unused variables\ndel x_train_NH_norm, y_train_NH_norm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import (\n    AveragePooling2D,\n    MaxPooling2D,\n    BatchNormalization,\n    Conv2D,\n    Conv2DTranspose,\n    Dense,\n    Dropout,\n    Flatten,\n    Lambda,\n    Layer,\n    LeakyReLU,\n    Reshape,\n)\n\n\ndef create_model_vgg_mod(input_dim):\n    def conv2(inputs, filters, kernel_size):\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(inputs)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = AveragePooling2D(padding=\"same\")(x)\n        return x\n    def conv3(inputs, filters, kernel_size):\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(inputs)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = AveragePooling2D(padding=\"same\")(x)\n        return x\n\n    inputs = Input(shape=input_dim)\n    x = conv2(inputs, filters=4, kernel_size=(2,2))\n    x = conv2(x, filters=8, kernel_size=(2,2))\n#     x = conv3(x, filters=8, kernel_size=(2,2))\n#     x = conv3(x, filters=16, kernel_size=(2,2))\n    x = conv2(x, filters=16, kernel_size=(2,2))\n    x = Flatten()(x)\n    x = Dense(64, kernel_regularizer=regularizers.l2(0.005))(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(1, kernel_regularizer=regularizers.l2(0.005), activation=\"linear\")(x)\n    VGG_mod = Model(inputs, x, name=\"vgg\")\n    VGG_mod.summary()\n\n    model_inputs = Input(shape=input_dim)\n    model_outputs = VGG_mod(model_inputs)\n    vgg_mod = Model(inputs=model_inputs, outputs=model_outputs)\n    return vgg_mod\n\n\nfrom tensorflow.keras.optimizers import Adam\n\n# Declare the model\nvgg = create_model_vgg_mod(\n    input_dim=(x_train.shape[1], x_train.shape[2], 1)\n)\n\n# Compile the model\nvgg.compile(optimizer=Adam(1e-5), loss=\"huber\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# train\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.005,\n    patience=30,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=False,\n)\n\nvgg.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    epochs=256,\n    batch_size=128,\n    callbacks=[early_stopping],\n    shuffle=True,\n    verbose=2,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the loss function\nfig = plt.figure(figsize=(8, 5), dpi=120)\nhistory = vgg.history.history\nplt.plot(history[\"loss\"], lw=2.5, label=\"Train\", alpha=0.8)\nplt.plot(history[\"val_loss\"], lw=2.5, label=\"Validation\", alpha=0.8)\nplt.title(\"Epoch vs Huber loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss (Huber)\")\nplt.legend(loc=\"best\")\nplt.savefig(\"CVAE_loss.png\")\nplt.close()\n\n# save model\nvgg.save(\"./mod_vgg.h5\")\n\n# test predictions with scatter plot\nx = vgg.predict(x_val)\nplt.scatter(x, y_val, s=1, alpha=0.05, label=\"data\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Pred\")\nplt.ylabel(\"True\")\nplt.axis('equal')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}